{"paper_id": "B_001", "title": "Long Short-Term Memory", "phase": "early", "background": "We study limitations of prior recurrence approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines recurrence with attention. We use a residual-style update F(x) = H(x) - x and train 11 layers. Keyword anchors: recurrence, attention, b_term_1.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary attention and measure robustness.", "results": {"metric_main": 0.565, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": [], "keywords": ["recurrence", "attention", "b_term_1"], "year": 2001, "venue": "SYNTH_CONF", "authors": ["Author_B1", "CoAuthor_B1"]}
{"paper_id": "B_002", "title": "Learning Phrase Representations using RNN Encoder-Decoder", "phase": "early", "background": "We study limitations of prior attention approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines attention with transformer. We use a residual-style update F(x) = H(x) - x and train 12 layers. Keyword anchors: attention, transformer, b_term_2.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary transformer and measure robustness.", "results": {"metric_main": 0.58, "delta_vs_prev": 0.022, "extra": {}}, "dependencies": ["B_001"], "keywords": ["attention", "transformer", "b_term_2"], "year": 2002, "venue": "SYNTH_CONF", "authors": ["Author_B2", "CoAuthor_B2"]}
{"paper_id": "B_003", "title": "Sequence to Sequence Learning with Neural Networks", "phase": "early", "background": "We study limitations of prior transformer approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines transformer with pretraining. We use a residual-style update F(x) = H(x) - x and train 13 layers. Keyword anchors: transformer, pretraining, b_term_3.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary pretraining and measure robustness.", "results": {"metric_main": 0.595, "delta_vs_prev": 0.023, "extra": {}}, "dependencies": ["B_002"], "keywords": ["transformer", "pretraining", "b_term_3"], "year": 2003, "venue": "SYNTH_CONF", "authors": ["Author_B3", "CoAuthor_B3"]}
{"paper_id": "B_004", "title": "Neural Machine Translation by Jointly Learning to Align and Translate", "phase": "early", "background": "We study limitations of prior pretraining approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines pretraining with instruction. We use a residual-style update F(x) = H(x) - x and train 14 layers. Keyword anchors: pretraining, instruction, b_term_4.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary instruction and measure robustness.", "results": {"metric_main": 0.61, "delta_vs_prev": 0.024, "extra": {}}, "dependencies": ["B_003"], "keywords": ["pretraining", "instruction", "b_term_4"], "year": 2004, "venue": "SYNTH_CONF", "authors": ["Author_B4", "CoAuthor_B4"]}
{"paper_id": "B_005", "title": "Effective Approaches to Attention-based Neural Machine Translation", "phase": "early", "background": "We study limitations of prior instruction approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines instruction with retrieval. We use a residual-style update F(x) = H(x) - x and train 15 layers. Keyword anchors: instruction, retrieval, b_term_5.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary retrieval and measure robustness.", "results": {"metric_main": 0.625, "delta_vs_prev": 0.025, "extra": {}}, "dependencies": ["B_004"], "keywords": ["instruction", "retrieval", "b_term_5"], "year": 2005, "venue": "SYNTH_CONF", "authors": ["Author_B5", "CoAuthor_B5"]}
{"paper_id": "B_006", "title": "Attention Is All You Need", "phase": "early", "background": "We study limitations of prior retrieval approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines retrieval with alignment. We use a residual-style update F(x) = H(x) - x and train 16 layers. Keyword anchors: retrieval, alignment, b_term_6.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary alignment and measure robustness.", "results": {"metric_main": 0.64, "delta_vs_prev": 0.026, "extra": {}}, "dependencies": ["B_005"], "keywords": ["retrieval", "alignment", "b_term_6"], "year": 2006, "venue": "SYNTH_CONF", "authors": ["Author_B6", "CoAuthor_B6"]}
{"paper_id": "B_007", "title": "Improving Language Understanding by Generative Pre-Training", "phase": "mid", "background": "We study limitations of prior alignment approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines alignment with efficient. We use a residual-style update F(x) = H(x) - x and train 17 layers. Keyword anchors: alignment, efficient, b_term_7.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary efficient and measure robustness.", "results": {"metric_main": 0.655, "delta_vs_prev": 0.027, "extra": {}}, "dependencies": ["B_006"], "keywords": ["alignment", "efficient", "b_term_7"], "year": 2007, "venue": "SYNTH_CONF", "authors": ["Author_B7", "CoAuthor_B7"]}
{"paper_id": "B_008", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "phase": "mid", "background": "We study limitations of prior efficient approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines efficient with reasoning. We use a residual-style update F(x) = H(x) - x and train 18 layers. Keyword anchors: efficient, reasoning, b_term_8.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary reasoning and measure robustness.", "results": {"metric_main": 0.67, "delta_vs_prev": 0.028, "extra": {}}, "dependencies": ["B_007"], "keywords": ["efficient", "reasoning", "b_term_8"], "year": 2008, "venue": "SYNTH_CONF", "authors": ["Author_B8", "CoAuthor_B8"]}
{"paper_id": "B_009", "title": "Language Models are Unsupervised Multitask Learners", "phase": "mid", "background": "We study limitations of prior reasoning approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines reasoning with tool-use. We use a residual-style update F(x) = H(x) - x and train 19 layers. Keyword anchors: reasoning, tool-use, b_term_9.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary tool-use and measure robustness.", "results": {"metric_main": 0.685, "delta_vs_prev": 0.029, "extra": {}}, "dependencies": ["B_008"], "keywords": ["reasoning", "tool-use", "b_term_9"], "year": 2009, "venue": "SYNTH_CONF", "authors": ["Author_B9", "CoAuthor_B9"]}
{"paper_id": "B_010", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "phase": "mid", "background": "We study limitations of prior tool-use approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines tool-use with recurrence. We use a residual-style update F(x) = H(x) - x and train 20 layers. Keyword anchors: tool-use, recurrence, b_term_10.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary recurrence and measure robustness.", "results": {"metric_main": 0.7, "delta_vs_prev": 0.03, "extra": {}}, "dependencies": ["B_009"], "keywords": ["tool-use", "recurrence", "b_term_10"], "year": 2010, "venue": "SYNTH_CONF", "authors": ["Author_B10", "CoAuthor_B10"]}
{"paper_id": "B_011", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "phase": "mid", "background": "We study limitations of prior recurrence approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines recurrence with attention. We use a residual-style update F(x) = H(x) - x and train 21 layers. Keyword anchors: recurrence, attention, b_term_11.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary attention and measure robustness.", "results": {"metric_main": 0.715, "delta_vs_prev": 0.031, "extra": {}}, "dependencies": ["B_010"], "keywords": ["recurrence", "attention", "b_term_11"], "year": 2011, "venue": "SYNTH_CONF", "authors": ["Author_B11", "CoAuthor_B11"]}
{"paper_id": "B_012", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "phase": "mid", "background": "We study limitations of prior attention approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines attention with transformer. We use a residual-style update F(x) = H(x) - x and train 22 layers. Keyword anchors: attention, transformer, b_term_12.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary transformer and measure robustness.", "results": {"metric_main": 0.73, "delta_vs_prev": 0.032, "extra": {}}, "dependencies": ["B_011"], "keywords": ["attention", "transformer", "b_term_12"], "year": 2012, "venue": "SYNTH_CONF", "authors": ["Author_B12", "CoAuthor_B12"]}
{"paper_id": "B_013", "title": "Language Models are Few-Shot Learners", "phase": "mid", "background": "We study limitations of prior transformer approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines transformer with pretraining. We use a residual-style update F(x) = H(x) - x and train 23 layers. Keyword anchors: transformer, pretraining, b_term_13.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary pretraining and measure robustness.", "results": {"metric_main": 0.745, "delta_vs_prev": 0.033, "extra": {}}, "dependencies": ["B_012"], "keywords": ["transformer", "pretraining", "b_term_13"], "year": 2013, "venue": "SYNTH_CONF", "authors": ["Author_B13", "CoAuthor_B13"]}
{"paper_id": "B_014", "title": "Switch Transformers: Scaling to Trillion Parameter Models", "phase": "late", "background": "We study limitations of prior pretraining approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines pretraining with instruction. We use a residual-style update F(x) = H(x) - x and train 24 layers. Keyword anchors: pretraining, instruction, b_term_14.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary instruction and measure robustness.", "results": {"metric_main": 0.76, "delta_vs_prev": 0.034, "extra": {}}, "dependencies": ["B_013"], "keywords": ["pretraining", "instruction", "b_term_14"], "year": 2014, "venue": "SYNTH_CONF", "authors": ["Author_B14", "CoAuthor_B14"]}
{"paper_id": "B_015", "title": "LoRA: Low-Rank Adaptation of Large Language Models", "phase": "late", "background": "We study limitations of prior instruction approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines instruction with retrieval. We use a residual-style update F(x) = H(x) - x and train 25 layers. Keyword anchors: instruction, retrieval, b_term_15.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary retrieval and measure robustness.", "results": {"metric_main": 0.775, "delta_vs_prev": 0.035, "extra": {}}, "dependencies": ["B_014"], "keywords": ["instruction", "retrieval", "b_term_15"], "year": 2015, "venue": "SYNTH_CONF", "authors": ["Author_B15", "CoAuthor_B15"]}
{"paper_id": "B_016", "title": "Training language models to follow instructions with human feedback", "phase": "late", "background": "We study limitations of prior retrieval approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines retrieval with alignment. We use a residual-style update F(x) = H(x) - x and train 26 layers. Keyword anchors: retrieval, alignment, b_term_16.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary alignment and measure robustness.", "results": {"metric_main": 0.79, "delta_vs_prev": 0.036, "extra": {}}, "dependencies": ["B_015"], "keywords": ["retrieval", "alignment", "b_term_16"], "year": 2016, "venue": "SYNTH_CONF", "authors": ["Author_B16", "CoAuthor_B16"]}
{"paper_id": "B_017", "title": "Constitutional AI: Harmlessness from AI Feedback", "phase": "late", "background": "We study limitations of prior alignment approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines alignment with efficient. We use a residual-style update F(x) = H(x) - x and train 27 layers. Keyword anchors: alignment, efficient, b_term_17.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary efficient and measure robustness.", "results": {"metric_main": 0.805, "delta_vs_prev": 0.037, "extra": {}}, "dependencies": ["B_016"], "keywords": ["alignment", "efficient", "b_term_17"], "year": 2017, "venue": "SYNTH_CONF", "authors": ["Author_B17", "CoAuthor_B17"]}
{"paper_id": "B_018", "title": "LLaMA: Open and Efficient Foundation Language Models", "phase": "late", "background": "We study limitations of prior efficient approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines efficient with reasoning. We use a residual-style update F(x) = H(x) - x and train 28 layers. Keyword anchors: efficient, reasoning, b_term_18.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary reasoning and measure robustness.", "results": {"metric_main": 0.82, "delta_vs_prev": 0.038, "extra": {}}, "dependencies": ["B_017"], "keywords": ["efficient", "reasoning", "b_term_18"], "year": 2018, "venue": "SYNTH_CONF", "authors": ["Author_B18", "CoAuthor_B18"]}
{"paper_id": "B_019", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "phase": "late", "background": "We study limitations of prior reasoning approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines reasoning with tool-use. We use a residual-style update F(x) = H(x) - x and train 29 layers. Keyword anchors: reasoning, tool-use, b_term_19.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary tool-use and measure robustness.", "results": {"metric_main": 0.835, "delta_vs_prev": 0.039, "extra": {}}, "dependencies": ["B_018"], "keywords": ["reasoning", "tool-use", "b_term_19"], "year": 2019, "venue": "SYNTH_CONF", "authors": ["Author_B19", "CoAuthor_B19"]}
{"paper_id": "B_020", "title": "Toolformer: Language Models Can Teach Themselves to Use Tools", "phase": "late", "background": "We study limitations of prior tool-use approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines tool-use with recurrence. We use a residual-style update F(x) = H(x) - x and train 30 layers. Keyword anchors: tool-use, recurrence, b_term_20.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary recurrence and measure robustness.", "results": {"metric_main": 0.85, "delta_vs_prev": 0.04, "extra": {}}, "dependencies": ["B_019"], "keywords": ["tool-use", "recurrence", "b_term_20"], "year": 2020, "venue": "SYNTH_CONF", "authors": ["Author_B20", "CoAuthor_B20"]}
