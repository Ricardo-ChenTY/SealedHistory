{"paper_id": "B_001", "title": "Long Short-Term Memory", "phase": "early", "background": "[TODO: fill manually]", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": [], "keywords": [], "year": null, "venue": null, "authors": null}
{"paper_id": "B_002", "title": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation", "phase": "early", "background": "In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The enco", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_001"], "keywords": [], "year": 2014, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": ["Kyunghyun Cho", "B. V. Merrienboer", "Çaglar Gülçehre"]}
{"paper_id": "B_003", "title": "Sequence to Sequence Learning with Neural Networks", "phase": "early", "background": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_001"], "keywords": [], "year": 2014, "venue": "Neural Information Processing Systems", "authors": ["I. Sutskever", "O. Vinyals", "Quoc V. Le"]}
{"paper_id": "B_004", "title": "Neural Machine Translation by Jointly Learning to Align and Translate", "phase": "early", "background": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed re", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_003"], "keywords": [], "year": 2014, "venue": "International Conference on Learning Representations", "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"]}
{"paper_id": "B_005", "title": "Effective Approaches to Attention-based Neural Machine Translation", "phase": "early", "background": "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effecti", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_004"], "keywords": [], "year": 2015, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": ["Thang Luong", "Hieu Pham", "Christopher D. Manning"]}
{"paper_id": "B_006", "title": "Attention is All you Need", "phase": "mid", "background": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_004", "B_005"], "keywords": [], "year": 2017, "venue": "Neural Information Processing Systems", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar"]}
{"paper_id": "B_007", "title": "Improving Language Understanding by Generative Pre-Training", "phase": "mid", "background": "[TODO: fill manually]", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_006"], "keywords": [], "year": null, "venue": null, "authors": null}
{"paper_id": "B_008", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "phase": "mid", "background": "[TODO: fill manually]", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_006"], "keywords": [], "year": null, "venue": null, "authors": null}
{"paper_id": "B_009", "title": "Language Models are Unsupervised Multitask Learners", "phase": "mid", "background": "[TODO: fill manually]", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_007"], "keywords": [], "year": null, "venue": null, "authors": null}
{"paper_id": "B_010", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "phase": "mid", "background": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on th", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_008"], "keywords": [], "year": 2019, "venue": "arXiv.org", "authors": ["Yinhan Liu", "Myle Ott", "Naman Goyal"]}
{"paper_id": "B_011", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "phase": "mid", "background": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two param", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_008"], "keywords": [], "year": 2019, "venue": "International Conference on Learning Representations", "authors": ["Zhenzhong Lan", "Mingda Chen", "Sebastian Goodman"]}
{"paper_id": "B_012", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "phase": "mid", "background": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and pract", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_008"], "keywords": [], "year": 2019, "venue": "Journal of machine learning research", "authors": ["Colin Raffel", "Noam Shazeer", "Adam Roberts"]}
{"paper_id": "B_013", "title": "Language Models are Few-Shot Learners", "phase": "late", "background": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of t", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_009"], "keywords": [], "year": 2020, "venue": "Neural Information Processing Systems", "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder"]}
{"paper_id": "B_014", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity", "phase": "late", "background": "In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_012", "B_013"], "keywords": [], "year": 2021, "venue": "Journal of machine learning research", "authors": ["W. Fedus", "Barret Zoph", "Noam Shazeer"]}
{"paper_id": "B_015", "title": "LoRA: Low-Rank Adaptation of Large Language Models", "phase": "late", "background": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example ", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_013"], "keywords": [], "year": 2021, "venue": "International Conference on Learning Representations", "authors": ["Edward J. Hu", "Yelong Shen", "Phillip Wallis"]}
{"paper_id": "B_016", "title": "Training language models to follow instructions with human feedback", "phase": "late", "background": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we sho", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_013"], "keywords": [], "year": 2022, "venue": "Neural Information Processing Systems", "authors": ["Long Ouyang", "Jeff Wu", "Xu Jiang"]}
{"paper_id": "B_017", "title": "Constitutional AI: Harmlessness from AI Feedback", "phase": "late", "background": "As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules o", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_016"], "keywords": [], "year": 2022, "venue": "arXiv.org", "authors": ["Yuntao Bai", "Saurav Kadavath", "Sandipan Kundu"]}
{"paper_id": "B_018", "title": "LLaMA: Open and Efficient Foundation Language Models", "phase": "late", "background": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_013"], "keywords": [], "year": 2023, "venue": "arXiv.org", "authors": ["Hugo Touvron", "Thibaut Lavril", "Gautier Izacard"]}
{"paper_id": "B_019", "title": "Scaling Data-Constrained Language Models", "phase": "late", "background": "The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling lang", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_013", "B_018"], "keywords": [], "year": 2023, "venue": "Neural Information Processing Systems", "authors": ["Niklas Muennighoff", "Alexander M. Rush", "B. Barak"]}
{"paper_id": "B_020", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "phase": "late", "background": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_008", "B_013"], "keywords": [], "year": 2020, "venue": "Neural Information Processing Systems", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandara Piktus"]}
{"paper_id": "B_021", "title": "Toolformer: Language Models Can Teach Themselves to Use Tools", "phase": "late", "background": "[TODO: fill manually]", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_013"], "keywords": [], "year": null, "venue": null, "authors": null}
{"paper_id": "B_022", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "phase": "late", "background": "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state ", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_006"], "keywords": [], "year": 2023, "venue": "arXiv.org", "authors": ["Albert Gu", "Tri Dao"]}
{"paper_id": "B_023", "title": "Efficiently Modeling Long Sequences with Structured State Spaces", "phase": "late", "background": "A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long ", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_006"], "keywords": [], "year": 2021, "venue": "International Conference on Learning Representations", "authors": ["Albert Gu", "Karan Goel", "Christopher R'e"]}
{"paper_id": "B_024", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", "phase": "late", "background": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not ach", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_006"], "keywords": [], "year": 2022, "venue": "Neural Information Processing Systems", "authors": ["Tri Dao", "Daniel Y. Fu", "Stefano Ermon"]}
{"paper_id": "B_025", "title": "Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models", "phase": "late", "background": "Sparse Mixture-of-Experts (MoE) is a neural architecture design that can be utilized to add learnable parameters to Large Language Models (LLMs) without increasing inference cost. Instruction tuning is a technique for training LLMs to follow instructions. We advocate combining these two approaches, ", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_014", "B_016"], "keywords": [], "year": 2023, "venue": "International Conference on Learning Representations", "authors": ["Sheng Shen", "Le Hou", "Yan-Quan Zhou"]}
{"paper_id": "B_026", "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model", "phase": "late", "background": "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_016"], "keywords": [], "year": 2023, "venue": "Neural Information Processing Systems", "authors": ["Rafael Rafailov", "Archit Sharma", "E. Mitchell"]}
{"paper_id": "B_027", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "phase": "late", "background": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simpl", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_013"], "keywords": [], "year": 2022, "venue": "Neural Information Processing Systems", "authors": ["Jason Wei", "Xuezhi Wang", "Dale Schuurmans"]}
{"paper_id": "B_028", "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "phase": "late", "background": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where i", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_027"], "keywords": [], "year": 2023, "venue": "Neural Information Processing Systems", "authors": ["Shunyu Yao", "Dian Yu", "Jeffrey Zhao"]}
{"paper_id": "B_029", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions", "phase": "late", "background": "Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, theref", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_016"], "keywords": [], "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": ["Yizhong Wang", "Yeganeh Kordi", "Swaroop Mishra"]}
{"paper_id": "B_030", "title": "Textbooks Are All You Need", "phase": "late", "background": "We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality\"data from the web (6B tokens) and synthetically generated te", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_018", "B_019"], "keywords": [], "year": 2023, "venue": "arXiv.org", "authors": ["Suriya Gunasekar", "Yi Zhang", "J. Aneja"]}
