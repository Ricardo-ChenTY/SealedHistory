{"paper_id": "A_001", "title": "Gradient-based learning applied to document recognition", "phase": "early", "background": "We study limitations of prior convolution approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines convolution with normalization. We use a residual-style update F(x) = H(x) - x and train 11 layers. Keyword anchors: convolution, normalization, a_term_1.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary normalization and measure robustness.", "results": {"metric_main": 0.565, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": [], "keywords": ["convolution", "normalization", "a_term_1"], "year": 2001, "venue": "SYNTH_CONF", "authors": ["Author_A1", "CoAuthor_A1"]}
{"paper_id": "A_002", "title": "ImageNet Classification with Deep Convolutional Neural Networks", "phase": "early", "background": "We study limitations of prior normalization approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines normalization with residual. We use a residual-style update F(x) = H(x) - x and train 12 layers. Keyword anchors: normalization, residual, a_term_2.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary residual and measure robustness.", "results": {"metric_main": 0.58, "delta_vs_prev": 0.022, "extra": {}}, "dependencies": ["A_001"], "keywords": ["normalization", "residual", "a_term_2"], "year": 2002, "venue": "SYNTH_CONF", "authors": ["Author_A2", "CoAuthor_A2"]}
{"paper_id": "A_003", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "phase": "early", "background": "We study limitations of prior residual approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines residual with transformer. We use a residual-style update F(x) = H(x) - x and train 13 layers. Keyword anchors: residual, transformer, a_term_3.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary transformer and measure robustness.", "results": {"metric_main": 0.595, "delta_vs_prev": 0.023, "extra": {}}, "dependencies": ["A_002"], "keywords": ["residual", "transformer", "a_term_3"], "year": 2003, "venue": "SYNTH_CONF", "authors": ["Author_A3", "CoAuthor_A3"]}
{"paper_id": "A_004", "title": "Network In Network", "phase": "early", "background": "We study limitations of prior transformer approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines transformer with self-supervised. We use a residual-style update F(x) = H(x) - x and train 14 layers. Keyword anchors: transformer, self-supervised, a_term_4.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary self-supervised and measure robustness.", "results": {"metric_main": 0.61, "delta_vs_prev": 0.024, "extra": {}}, "dependencies": ["A_003"], "keywords": ["transformer", "self-supervised", "a_term_4"], "year": 2004, "venue": "SYNTH_CONF", "authors": ["Author_A4", "CoAuthor_A4"]}
{"paper_id": "A_005", "title": "Going Deeper with Convolutions", "phase": "early", "background": "We study limitations of prior self-supervised approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines self-supervised with contrastive. We use a residual-style update F(x) = H(x) - x and train 15 layers. Keyword anchors: self-supervised, contrastive, a_term_5.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary contrastive and measure robustness.", "results": {"metric_main": 0.625, "delta_vs_prev": 0.025, "extra": {}}, "dependencies": ["A_004"], "keywords": ["self-supervised", "contrastive", "a_term_5"], "year": 2005, "venue": "SYNTH_CONF", "authors": ["Author_A5", "CoAuthor_A5"]}
{"paper_id": "A_006", "title": "Batch Normalization: Accelerating Deep Network Training", "phase": "early", "background": "We study limitations of prior contrastive approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines contrastive with detection. We use a residual-style update F(x) = H(x) - x and train 16 layers. Keyword anchors: contrastive, detection, a_term_6.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary detection and measure robustness.", "results": {"metric_main": 0.64, "delta_vs_prev": 0.026, "extra": {}}, "dependencies": ["A_005"], "keywords": ["contrastive", "detection", "a_term_6"], "year": 2006, "venue": "SYNTH_CONF", "authors": ["Author_A6", "CoAuthor_A6"]}
{"paper_id": "A_007", "title": "Deep Residual Learning for Image Recognition", "phase": "mid", "background": "We study limitations of prior detection approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines detection with segmentation. We use a residual-style update F(x) = H(x) - x and train 17 layers. Keyword anchors: detection, segmentation, a_term_7.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary segmentation and measure robustness.", "results": {"metric_main": 0.655, "delta_vs_prev": 0.027, "extra": {}}, "dependencies": ["A_006"], "keywords": ["detection", "segmentation", "a_term_7"], "year": 2007, "venue": "SYNTH_CONF", "authors": ["Author_A7", "CoAuthor_A7"]}
{"paper_id": "A_008", "title": "Densely Connected Convolutional Networks", "phase": "mid", "background": "We study limitations of prior segmentation approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines segmentation with distillation. We use a residual-style update F(x) = H(x) - x and train 18 layers. Keyword anchors: segmentation, distillation, a_term_8.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary distillation and measure robustness.", "results": {"metric_main": 0.67, "delta_vs_prev": 0.028, "extra": {}}, "dependencies": ["A_007"], "keywords": ["segmentation", "distillation", "a_term_8"], "year": 2008, "venue": "SYNTH_CONF", "authors": ["Author_A8", "CoAuthor_A8"]}
{"paper_id": "A_009", "title": "Squeeze-and-Excitation Networks", "phase": "mid", "background": "We study limitations of prior distillation approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines distillation with scaling. We use a residual-style update F(x) = H(x) - x and train 19 layers. Keyword anchors: distillation, scaling, a_term_9.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary scaling and measure robustness.", "results": {"metric_main": 0.685, "delta_vs_prev": 0.029, "extra": {}}, "dependencies": ["A_008"], "keywords": ["distillation", "scaling", "a_term_9"], "year": 2009, "venue": "SYNTH_CONF", "authors": ["Author_A9", "CoAuthor_A9"]}
{"paper_id": "A_010", "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications", "phase": "mid", "background": "We study limitations of prior scaling approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines scaling with convolution. We use a residual-style update F(x) = H(x) - x and train 20 layers. Keyword anchors: scaling, convolution, a_term_10.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary convolution and measure robustness.", "results": {"metric_main": 0.7, "delta_vs_prev": 0.03, "extra": {}}, "dependencies": ["A_009"], "keywords": ["scaling", "convolution", "a_term_10"], "year": 2010, "venue": "SYNTH_CONF", "authors": ["Author_A10", "CoAuthor_A10"]}
{"paper_id": "A_011", "title": "Neural Architecture Search with Reinforcement Learning", "phase": "mid", "background": "We study limitations of prior convolution approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines convolution with normalization. We use a residual-style update F(x) = H(x) - x and train 21 layers. Keyword anchors: convolution, normalization, a_term_11.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary normalization and measure robustness.", "results": {"metric_main": 0.715, "delta_vs_prev": 0.031, "extra": {}}, "dependencies": ["A_010"], "keywords": ["convolution", "normalization", "a_term_11"], "year": 2011, "venue": "SYNTH_CONF", "authors": ["Author_A11", "CoAuthor_A11"]}
{"paper_id": "A_012", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks", "phase": "mid", "background": "We study limitations of prior normalization approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines normalization with residual. We use a residual-style update F(x) = H(x) - x and train 22 layers. Keyword anchors: normalization, residual, a_term_12.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary residual and measure robustness.", "results": {"metric_main": 0.73, "delta_vs_prev": 0.032, "extra": {}}, "dependencies": ["A_011"], "keywords": ["normalization", "residual", "a_term_12"], "year": 2012, "venue": "SYNTH_CONF", "authors": ["Author_A12", "CoAuthor_A12"]}
{"paper_id": "A_013", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "phase": "mid", "background": "We study limitations of prior residual approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines residual with transformer. We use a residual-style update F(x) = H(x) - x and train 23 layers. Keyword anchors: residual, transformer, a_term_13.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary transformer and measure robustness.", "results": {"metric_main": 0.745, "delta_vs_prev": 0.033, "extra": {}}, "dependencies": ["A_012"], "keywords": ["residual", "transformer", "a_term_13"], "year": 2013, "venue": "SYNTH_CONF", "authors": ["Author_A13", "CoAuthor_A13"]}
{"paper_id": "A_014", "title": "Training data-efficient image transformers & distillation through attention", "phase": "late", "background": "We study limitations of prior transformer approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines transformer with self-supervised. We use a residual-style update F(x) = H(x) - x and train 24 layers. Keyword anchors: transformer, self-supervised, a_term_14.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary self-supervised and measure robustness.", "results": {"metric_main": 0.76, "delta_vs_prev": 0.034, "extra": {}}, "dependencies": ["A_013"], "keywords": ["transformer", "self-supervised", "a_term_14"], "year": 2014, "venue": "SYNTH_CONF", "authors": ["Author_A14", "CoAuthor_A14"]}
{"paper_id": "A_015", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows", "phase": "late", "background": "We study limitations of prior self-supervised approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines self-supervised with contrastive. We use a residual-style update F(x) = H(x) - x and train 25 layers. Keyword anchors: self-supervised, contrastive, a_term_15.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary contrastive and measure robustness.", "results": {"metric_main": 0.775, "delta_vs_prev": 0.035, "extra": {}}, "dependencies": ["A_014"], "keywords": ["self-supervised", "contrastive", "a_term_15"], "year": 2015, "venue": "SYNTH_CONF", "authors": ["Author_A15", "CoAuthor_A15"]}
{"paper_id": "A_016", "title": "Masked Autoencoders Are Scalable Vision Learners", "phase": "late", "background": "We study limitations of prior contrastive approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines contrastive with detection. We use a residual-style update F(x) = H(x) - x and train 26 layers. Keyword anchors: contrastive, detection, a_term_16.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary detection and measure robustness.", "results": {"metric_main": 0.79, "delta_vs_prev": 0.036, "extra": {}}, "dependencies": ["A_015"], "keywords": ["contrastive", "detection", "a_term_16"], "year": 2016, "venue": "SYNTH_CONF", "authors": ["Author_A16", "CoAuthor_A16"]}
{"paper_id": "A_017", "title": "Learning Transferable Visual Models From Natural Language Supervision", "phase": "late", "background": "We study limitations of prior detection approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines detection with segmentation. We use a residual-style update F(x) = H(x) - x and train 27 layers. Keyword anchors: detection, segmentation, a_term_17.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary segmentation and measure robustness.", "results": {"metric_main": 0.805, "delta_vs_prev": 0.037, "extra": {}}, "dependencies": ["A_016"], "keywords": ["detection", "segmentation", "a_term_17"], "year": 2017, "venue": "SYNTH_CONF", "authors": ["Author_A17", "CoAuthor_A17"]}
{"paper_id": "A_018", "title": "A ConvNet for the 2020s", "phase": "late", "background": "We study limitations of prior segmentation approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines segmentation with distillation. We use a residual-style update F(x) = H(x) - x and train 28 layers. Keyword anchors: segmentation, distillation, a_term_18.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary distillation and measure robustness.", "results": {"metric_main": 0.82, "delta_vs_prev": 0.038, "extra": {}}, "dependencies": ["A_017"], "keywords": ["segmentation", "distillation", "a_term_18"], "year": 2018, "venue": "SYNTH_CONF", "authors": ["Author_A18", "CoAuthor_A18"]}
{"paper_id": "A_019", "title": "Scaling Vision Transformers to 22 Billion Parameters", "phase": "late", "background": "We study limitations of prior distillation approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines distillation with scaling. We use a residual-style update F(x) = H(x) - x and train 29 layers. Keyword anchors: distillation, scaling, a_term_19.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary scaling and measure robustness.", "results": {"metric_main": 0.835, "delta_vs_prev": 0.039, "extra": {}}, "dependencies": ["A_018"], "keywords": ["distillation", "scaling", "a_term_19"], "year": 2019, "venue": "SYNTH_CONF", "authors": ["Author_A19", "CoAuthor_A19"]}
{"paper_id": "A_020", "title": "Segment Anything", "phase": "late", "background": "We study limitations of prior scaling approaches and propose a more reliable setup. Key challenge: stable generalization under distribution shift.", "mechanism": "Our core mechanism combines scaling with convolution. We use a residual-style update F(x) = H(x) - x and train 30 layers. Keyword anchors: scaling, convolution, a_term_20.", "experiment": "We evaluate on a synthetic benchmark and report a main score. Ablations vary convolution and measure robustness.", "results": {"metric_main": 0.85, "delta_vs_prev": 0.04, "extra": {}}, "dependencies": ["A_019"], "keywords": ["scaling", "convolution", "a_term_20"], "year": 2020, "venue": "SYNTH_CONF", "authors": ["Author_A20", "CoAuthor_A20"]}
