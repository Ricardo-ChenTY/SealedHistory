{"paper_id":"A_001","title":"Gradient-Based Feature Detectors","phase":"early","background":"Hand-crafted features (SIFT, HOG) dominate visual recognition but require domain expertise and scale poorly to large datasets.","mechanism":"Train a shallow neural network with back-propagation on pixel patches; learn convolutional filters automatically from data. Use weight sharing across spatial locations.","experiment":"Digit recognition (10 classes, 60k train / 10k test). Compare learned filters vs hand-crafted. Metric: error rate.","results":{"metric_main":0.95,"delta_vs_prev":0.0},"dependencies":[],"keywords":["convolutional network","backpropagation","weight sharing","digit recognition","learned features"],"year":1998,"venue":"ConfA","authors":["Y. Lecun"]}
{"paper_id":"A_002","title":"Large-Scale Visual Recognition with Deep Convolutions","phase":"early","background":"Shallow networks plateau on complex image classification; deeper architectures underperform due to overfitting and slow training.","mechanism":"Stack multiple conv-pool layers (5 conv + 3 FC), use ReLU activations, dropout regularization, and GPU-parallel training. Data augmentation via crops and flips.","experiment":"1000-class image classification (1.2M train / 50k val). Metric: top-5 error rate. Ablation: depth, dropout, augmentation.","results":{"metric_main":0.843,"delta_vs_prev":0.10},"dependencies":["A_001"],"keywords":["deep convolution","ReLU","dropout","GPU training","image classification"],"year":2012,"venue":"ConfB","authors":["A. Krizhevsky"]}
{"paper_id":"A_003","title":"Very Deep Convolutional Networks","phase":"early","background":"Deeper networks should capture more abstract features, but training deeper architectures is difficult with existing techniques.","mechanism":"Use uniform 3x3 conv filters throughout; increase depth to 16-19 layers. Small filters reduce parameters while increasing receptive field through stacking.","experiment":"Same 1000-class benchmark as A_002. Metric: top-5 error. Ablation: depth (11/13/16/19 layers), filter size.","results":{"metric_main":0.927,"delta_vs_prev":0.084},"dependencies":["A_002"],"keywords":["very deep network","3x3 convolution","depth","feature hierarchy"],"year":2014,"venue":"ConfC","authors":["K. Simonyan"]}
{"paper_id":"A_004","title":"Network-in-Network with 1x1 Convolutions","phase":"early","background":"Standard conv layers are linear filters; adding non-linearity within each layer could improve discrimination without increasing depth.","mechanism":"Replace linear conv filters with micro multi-layer perceptrons (mlpconv). Use 1x1 convolutions as cross-channel feature combiners. Global average pooling instead of FC layers.","experiment":"Multiple benchmarks (10-class, 100-class, 1000-class). Metric: error rate. Ablation: mlpconv vs standard conv, global avg pooling vs FC.","results":{"metric_main":0.914,"delta_vs_prev":0.02},"dependencies":["A_002"],"keywords":["1x1 convolution","mlpconv","global average pooling","cross-channel"],"year":2014,"venue":"ConfD","authors":["M. Lin"]}
{"paper_id":"A_005","title":"Going Deeper with Inception Modules","phase":"mid","background":"Increasing depth and width naively leads to computational explosion; need architecturally efficient ways to scale networks.","mechanism":"Inception module: parallel branches of 1x1, 3x3, 5x5 conv and max-pool, concatenated. Use 1x1 bottlenecks to reduce computation. Stack 22 layers with auxiliary classifiers for gradient flow.","experiment":"1000-class benchmark. Metric: top-5 error. Ablation: module design, bottleneck reduction, auxiliary losses.","results":{"metric_main":0.933,"delta_vs_prev":0.006},"dependencies":["A_003","A_004"],"keywords":["inception module","multi-scale","bottleneck","auxiliary classifier"],"year":2015,"venue":"ConfE","authors":["C. Szegedy"]}
{"paper_id":"A_006","title":"Deep Residual Learning","phase":"mid","background":"Very deep networks suffer from degradation (not overfitting): deeper plain nets have higher training error. Identity mappings should be easy to learn but are not.","mechanism":"Shortcut/skip connections: learn residual F(x) = H(x) - x instead of H(x). Stack residual blocks (two 3x3 conv + skip). Enables training 152+ layer networks.","experiment":"1000-class benchmark. Metric: top-5 error. Ablation: depth (18/34/50/101/152), plain vs residual, bottleneck design.","results":{"metric_main":0.964,"delta_vs_prev":0.031},"dependencies":["A_003"],"keywords":["residual learning","skip connection","identity mapping","degradation problem","very deep"],"year":2015,"venue":"ConfE","authors":["K. He"]}
{"paper_id":"A_007","title":"Densely Connected Networks","phase":"mid","background":"Residual connections help gradient flow but only connect adjacent layers; denser connectivity patterns may further improve feature reuse and reduce parameters.","mechanism":"Each layer receives feature maps from ALL preceding layers (dense connectivity). Growth rate controls channel expansion. Transition layers (1x1 conv + pool) between dense blocks.","experiment":"Multiple benchmarks (10-class, 100-class). Metric: error rate. Ablation: growth rate, compression ratio, depth.","results":{"metric_main":0.953,"delta_vs_prev":0.005},"dependencies":["A_006"],"keywords":["dense connectivity","feature reuse","growth rate","transition layer"],"year":2017,"venue":"ConfE","authors":["G. Huang"]}
{"paper_id":"A_008","title":"Squeeze-and-Excitation Channel Attention","phase":"mid","background":"Convolutional features treat all channels equally; adaptively recalibrating channel-wise responses could improve representational power.","mechanism":"SE block: global average pool -> FC squeeze (reduce channels by ratio r) -> ReLU -> FC excite (restore channels) -> sigmoid -> channel-wise rescaling. Plug into any backbone.","experiment":"1000-class benchmark with multiple backbones. Metric: top-5 error. Ablation: reduction ratio r, insertion position.","results":{"metric_main":0.967,"delta_vs_prev":0.003},"dependencies":["A_006"],"keywords":["channel attention","squeeze","excitation","recalibration","plug-in module"],"year":2018,"venue":"ConfE","authors":["J. Hu"]}
{"paper_id":"A_009","title":"Neural Architecture Search for ConvNets","phase":"mid","background":"Manual network design is time-consuming and may miss optimal architectures; automated search in a defined space could outperform human designs.","mechanism":"Use reinforcement learning (or evolutionary) controller to sample architectures from a cell-based search space. Train child networks, use validation accuracy as reward. Transfer found cells to large-scale settings.","experiment":"Proxy task (small dataset) then transfer to 1000-class. Metric: top-5 error. Ablation: search space, search algorithm, transfer gap.","results":{"metric_main":0.968,"delta_vs_prev":0.001},"dependencies":["A_006","A_005"],"keywords":["neural architecture search","NAS","reinforcement learning","cell-based","automated design"],"year":2018,"venue":"ConfE","authors":["B. Zoph"]}
{"paper_id":"A_010","title":"EfficientNet: Compound Scaling","phase":"mid","background":"Scaling network depth, width, or resolution individually yields diminishing returns; a principled approach to joint scaling is lacking.","mechanism":"Compound scaling: uniformly scale depth (d), width (w), and resolution (r) using coefficients tied by a compound ratio phi. Use NAS-found baseline architecture, then scale up systematically.","experiment":"1000-class benchmark. Metric: top-5 accuracy, FLOPs. Ablation: individual vs compound scaling, different phi values.","results":{"metric_main":0.972,"delta_vs_prev":0.004},"dependencies":["A_009","A_006"],"keywords":["compound scaling","efficiency","depth-width-resolution","scaling law"],"year":2019,"venue":"ConfF","authors":["M. Tan"]}
{"paper_id":"A_011","title":"Vision Transformer: Patches as Tokens","phase":"late","background":"Transformers dominate NLP via self-attention but are unexplored for vision; CNNs remain default. Can pure attention replace convolutions?","mechanism":"Split image into fixed-size patches, linearly embed each patch as a token, prepend a [CLS] token, add positional embeddings, feed through standard Transformer encoder. No convolutions.","experiment":"Multiple benchmarks (1000-class, fine-grained). Metric: top-1 accuracy. Compare vs CNN baselines at various data scales. Ablation: patch size, model size, pre-training data.","results":{"metric_main":0.908,"delta_vs_prev":-0.064},"dependencies":["A_006"],"keywords":["vision transformer","patch embedding","self-attention","position embedding","no convolution"],"year":2020,"venue":"ConfG","authors":["A. Dosovitskiy"]}
{"paper_id":"A_012","title":"Data-efficient Image Transformers with Distillation","phase":"late","background":"Vision Transformers require massive pre-training data (300M+ images); training on standard datasets (1.2M images) yields poor results compared to CNNs.","mechanism":"Knowledge distillation from a CNN teacher: add a distillation token alongside the class token; train with both hard-label and soft-label distillation losses. Strong data augmentation (RandAugment, mixup, cutmix).","experiment":"1000-class benchmark, ImageNet-only training. Metric: top-1 accuracy. Ablation: distillation strategy, teacher model, augmentation.","results":{"metric_main":0.838,"delta_vs_prev":0.03},"dependencies":["A_011"],"keywords":["distillation","data-efficient","distillation token","augmentation","transformer training"],"year":2021,"venue":"ConfF","authors":["H. Touvron"]}
{"paper_id":"A_013","title":"Swin Transformer: Shifted Windows","phase":"late","background":"Standard ViT has quadratic complexity w.r.t. token count, limiting high-resolution applications; also lacks multi-scale feature hierarchy for dense prediction.","mechanism":"Compute self-attention within local windows; shift window partitions between layers for cross-window connectivity. Hierarchical design: patch merging reduces spatial resolution progressively, producing multi-scale features.","experiment":"1000-class classification + object detection + segmentation. Metric: top-1 acc / mAP / mIoU. Ablation: window size, shift strategy, model size.","results":{"metric_main":0.876,"delta_vs_prev":0.04},"dependencies":["A_011"],"keywords":["shifted window","local attention","hierarchical transformer","multi-scale","linear complexity"],"year":2021,"venue":"ConfH","authors":["Z. Liu"]}
{"paper_id":"A_014","title":"Masked Autoencoders for Visual Pre-training","phase":"late","background":"Self-supervised pre-training via contrastive learning requires augmentation engineering and negative samples; a simpler reconstructive objective akin to masked LM is missing for vision.","mechanism":"Mask a high ratio (75%) of image patches; encode only visible patches with a ViT encoder; decode with a lightweight decoder to reconstruct masked pixels. Asymmetric encoder-decoder design.","experiment":"Pre-train on 1.2M images, fine-tune on 1000-class and transfer tasks. Metric: top-1 accuracy. Ablation: mask ratio, decoder depth, reconstruction target.","results":{"metric_main":0.871,"delta_vs_prev":0.01},"dependencies":["A_011"],"keywords":["masked autoencoder","self-supervised","reconstruction","high mask ratio","asymmetric design"],"year":2022,"venue":"ConfE","authors":["K. He"]}
{"paper_id":"A_015","title":"Contrastive Language-Image Pre-training","phase":"late","background":"Vision models trained on fixed label sets lack open-world generalization; aligning visual representations with natural language could enable zero-shot transfer.","mechanism":"Dual-encoder: image encoder (ViT or CNN) + text encoder (Transformer). Train with contrastive loss on 400M image-text pairs: maximize cosine similarity of matched pairs, minimize unmatched. Zero-shot classification via text prompts.","experiment":"Zero-shot on 27 downstream datasets. Metric: accuracy without fine-tuning. Ablation: data scale, encoder architecture, prompt engineering.","results":{"metric_main":0.763,"delta_vs_prev":0.0},"dependencies":["A_011"],"keywords":["contrastive learning","vision-language","zero-shot","dual encoder","image-text alignment"],"year":2021,"venue":"ConfF","authors":["A. Radford"]}
{"paper_id":"A_016","title":"Scaling Visual Pre-training with Billions of Parameters","phase":"late","background":"Prior ViT scaling studies plateau at ~600M parameters; whether vision models benefit from scaling to billions (like LLMs) remains unclear.","mechanism":"Scale ViT to 2B+ parameters using modified training recipes: layer normalization placement, higher resolution, longer training schedules. Use knowledge distillation and model parallelism for efficient training.","experiment":"1000-class benchmark and transfer. Metric: top-1 accuracy. Ablation: model size (300M to 22B), training duration, distillation.","results":{"metric_main":0.909,"delta_vs_prev":0.001},"dependencies":["A_011","A_014"],"keywords":["scaling","billion parameter","large vision model","training recipe","model parallelism"],"year":2023,"venue":"ConfE","authors":["M. Dehghani"]}
{"paper_id":"A_017","title":"ConvNeXt: Modernizing ConvNets","phase":"late","background":"Swin Transformer outperforms ResNets, but is the gap due to attention or just modern training? Can pure ConvNets match Transformers with updated designs?","mechanism":"Start from ResNet, progressively adopt Transformer-era designs: patchify stem, inverted bottleneck, large kernel (7x7), fewer activation functions, layer normalization, GELU activation. Keep pure convolution.","experiment":"1000-class and downstream tasks. Metric: top-1 accuracy / mAP / mIoU. Ablation: each modernization step individually.","results":{"metric_main":0.874,"delta_vs_prev":0.01},"dependencies":["A_006","A_013"],"keywords":["modernized convnet","large kernel","inverted bottleneck","design space","convolution vs attention"],"year":2022,"venue":"ConfE","authors":["Z. Liu"]}
{"paper_id":"A_018","title":"Segment Anything: Foundation Model for Segmentation","phase":"late","background":"Segmentation models are task-specific and require per-dataset annotation; a promptable, general-purpose segmentation foundation model is missing.","mechanism":"Prompt-based segmentation: image encoder (ViT-H) produces embeddings; a lightweight prompt encoder accepts points/boxes/masks/text; a mask decoder outputs valid masks. Train on 1B+ masks via data engine.","experiment":"23 diverse segmentation datasets, zero-shot. Metric: mIoU. Ablation: prompt types, encoder size, data engine iterations.","results":{"metric_main":0.789,"delta_vs_prev":0.0},"dependencies":["A_011","A_015"],"keywords":["foundation model","promptable segmentation","data engine","zero-shot segmentation","billion masks"],"year":2023,"venue":"ConfH","authors":["A. Kirillov"]}
{"paper_id":"A_019","title":"DINOv2: Self-supervised Visual Features at Scale","phase":"late","background":"Self-supervised ViT features (DINO, MAE) are promising but trained only at moderate scale; scaling self-supervised pre-training to curated large data may produce universal visual features.","mechanism":"Combine DINO self-distillation with iBOT masked prediction. Curate 142M images via automated retrieval pipeline. Train ViT-g with stabilization techniques (LayerScale, stochastic depth, KoLeo regularizer).","experiment":"Linear probing and k-NN on 12 benchmarks. Metric: accuracy without fine-tuning. Ablation: data curation, model size, training objectives.","results":{"metric_main":0.862,"delta_vs_prev":0.02},"dependencies":["A_014","A_011"],"keywords":["self-distillation","self-supervised scaling","data curation","universal features","frozen features"],"year":2023,"venue":"ConfI","authors":["M. Oquab"]}
{"paper_id":"A_020","title":"Vision-Language Models with Interleaved Attention","phase":"late","background":"CLIP-style dual encoders lack fine-grained vision-language interaction; deeper fusion during pre-training could improve complex reasoning tasks.","mechanism":"Interleave cross-attention layers between a frozen LLM and a vision encoder. Feed image tokens at multiple layers rather than only as prefix. Pre-train on interleaved image-text web data with next-token prediction.","experiment":"VQA, captioning, and multi-modal benchmarks. Metric: accuracy / CIDEr. Ablation: cross-attention frequency, vision encoder choice, data mixture.","results":{"metric_main":0.821,"delta_vs_prev":0.03},"dependencies":["A_015","A_016"],"keywords":["interleaved attention","vision-language model","cross-attention","multi-modal","deep fusion"],"year":2023,"venue":"ConfI","authors":["J. Alayrac"]}
