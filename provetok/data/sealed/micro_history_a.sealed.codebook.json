{
  "seed": 42,
  "forward": {
    "convolutional network": "quark",
    "backpropagation": "prism",
    "weight sharing": "nexus",
    "digit recognition": "vertex",
    "learned features": "helix",
    "y. lecun": "J. Smith",
    "confa": "AICONF",
    "gradient-based feature detectors": "ZetaNet",
    "deep convolution": "lattice",
    "relu": "matrix",
    "dropout": "tensor",
    "gpu training": "vector",
    "image classification": "kernel",
    "a. krizhevsky": "R. Chen",
    "confb": "VISIONEX",
    "large-scale visual recognition with deep convolutions": "OmegaArch",
    "very deep network": "quark_1",
    "3x3 convolution": "prism_1",
    "depth": "nexus_1",
    "feature hierarchy": "vertex_1",
    "k. simonyan": "M. Ivanova",
    "confc": "LEARNCON",
    "very deep convolutional networks": "KappaModel",
    "1x1 convolution": "helix_1",
    "mlpconv": "lattice_1",
    "global average pooling": "matrix_1",
    "cross-channel": "tensor_1",
    "m. lin": "K. Tanaka",
    "confd": "NEUROSYM",
    "network-in-network with 1x1 convolutions": "SigmaBlock",
    "inception module": "vector_1",
    "multi-scale": "kernel_1",
    "bottleneck": "quark_2",
    "auxiliary classifier": "prism_2",
    "c. szegedy": "S. Patel",
    "confe": "DATAFORGE",
    "going deeper with inception modules": "PhiStack",
    "residual learning": "nexus_2",
    "skip connection": "vertex_2",
    "identity mapping": "helix_2",
    "degradation problem": "lattice_2",
    "very deep": "matrix_2",
    "k. he": "L. Garcia",
    "deep residual learning": "ThetaCore",
    "dense connectivity": "tensor_2",
    "feature reuse": "vector_2",
    "growth rate": "kernel_2",
    "transition layer": "quark_3",
    "g. huang": "A. Johansson",
    "densely connected networks": "LambdaUnit",
    "channel attention": "prism_3",
    "squeeze": "nexus_3",
    "excitation": "vertex_3",
    "recalibration": "helix_3",
    "plug-in module": "lattice_3",
    "j. hu": "D. Kowalski",
    "squeeze-and-excitation channel attention": "DeltaFrame",
    "neural architecture search": "matrix_3",
    "nas": "tensor_3",
    "reinforcement learning": "vector_3",
    "cell-based": "kernel_3",
    "automated design": "quark_4",
    "b. zoph": "W. Okafor",
    "neural architecture search for convnets": "EpsilonNode",
    "compound scaling": "prism_4",
    "efficiency": "nexus_4",
    "depth-width-resolution": "vertex_4",
    "scaling law": "helix_4",
    "m. tan": "P. Dubois",
    "conff": "SIGNALX",
    "efficientnet: compound scaling": "RhoNet",
    "vision transformer": "lattice_4",
    "patch embedding": "matrix_4",
    "self-attention": "tensor_4",
    "position embedding": "vector_4",
    "no convolution": "kernel_4",
    "a. dosovitskiy": "J. Smith_1",
    "confg": "OPTICON",
    "vision transformer: patches as tokens": "ZetaNet_1",
    "distillation": "quark_5",
    "data-efficient": "prism_5",
    "distillation token": "nexus_5",
    "augmentation": "vertex_5",
    "transformer training": "helix_5",
    "h. touvron": "R. Chen_1",
    "data-efficient image transformers with distillation": "OmegaArch_1",
    "shifted window": "lattice_5",
    "local attention": "matrix_5",
    "hierarchical transformer": "tensor_5",
    "linear complexity": "vector_5",
    "z. liu": "M. Ivanova_1",
    "confh": "COGNITA",
    "swin transformer: shifted windows": "KappaModel_1",
    "masked autoencoder": "kernel_5",
    "self-supervised": "quark_6",
    "reconstruction": "prism_6",
    "high mask ratio": "nexus_6",
    "asymmetric design": "vertex_6",
    "masked autoencoders for visual pre-training": "SigmaBlock_1",
    "contrastive learning": "helix_6",
    "vision-language": "lattice_6",
    "zero-shot": "matrix_6",
    "dual encoder": "tensor_6",
    "image-text alignment": "vector_6",
    "a. radford": "K. Tanaka_1",
    "contrastive language-image pre-training": "PhiStack_1",
    "scaling": "kernel_6",
    "billion parameter": "quark_7",
    "large vision model": "prism_7",
    "training recipe": "nexus_7",
    "model parallelism": "vertex_7",
    "m. dehghani": "S. Patel_1",
    "scaling visual pre-training with billions of parameters": "ThetaCore_1",
    "modernized convnet": "helix_7",
    "large kernel": "lattice_7",
    "inverted bottleneck": "matrix_7",
    "design space": "tensor_7",
    "convolution vs attention": "vector_7",
    "convnext: modernizing convnets": "LambdaUnit_1",
    "foundation model": "kernel_7",
    "promptable segmentation": "quark_8",
    "data engine": "prism_8",
    "zero-shot segmentation": "nexus_8",
    "billion masks": "vertex_8",
    "a. kirillov": "L. Garcia_1",
    "segment anything: foundation model for segmentation": "DeltaFrame_1",
    "self-distillation": "helix_8",
    "self-supervised scaling": "lattice_8",
    "data curation": "matrix_8",
    "universal features": "tensor_8",
    "frozen features": "vector_8",
    "m. oquab": "A. Johansson_1",
    "confi": "COMPULEARN",
    "dinov2: self-supervised visual features at scale": "EpsilonNode_1",
    "interleaved attention": "kernel_8",
    "vision-language model": "quark_9",
    "cross-attention": "prism_9",
    "multi-modal": "nexus_9",
    "deep fusion": "vertex_9",
    "j. alayrac": "D. Kowalski_1",
    "vision-language models with interleaved attention": "RhoNet_1"
  },
  "category_counters": {
    "keyword": 94,
    "author": 18,
    "venue": 9,
    "model": 20
  }
}