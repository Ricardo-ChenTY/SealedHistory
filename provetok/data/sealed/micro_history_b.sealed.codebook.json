{
  "seed": 42,
  "forward": {
    "recurrence": "quark",
    "attention": "prism",
    "b_term_1": "nexus",
    "author_b1": "J. Smith",
    "coauthor_b1": "R. Chen",
    "synth_conf": "AICONF",
    "long short-term memory": "ZetaNet",
    "transformer": "vertex",
    "b_term_2": "helix",
    "author_b2": "M. Ivanova",
    "coauthor_b2": "K. Tanaka",
    "learning phrase representations using rnn encoder-decoder": "OmegaArch",
    "pretraining": "lattice",
    "b_term_3": "matrix",
    "author_b3": "S. Patel",
    "coauthor_b3": "L. Garcia",
    "sequence to sequence learning with neural networks": "KappaModel",
    "instruction": "tensor",
    "b_term_4": "vector",
    "author_b4": "A. Johansson",
    "coauthor_b4": "D. Kowalski",
    "neural machine translation by jointly learning to align and translate": "SigmaBlock",
    "retrieval": "kernel",
    "b_term_5": "quark_1",
    "author_b5": "W. Okafor",
    "coauthor_b5": "P. Dubois",
    "effective approaches to attention-based neural machine translation": "PhiStack",
    "alignment": "prism_1",
    "b_term_6": "nexus_1",
    "author_b6": "J. Smith_1",
    "coauthor_b6": "R. Chen_1",
    "attention is all you need": "ThetaCore",
    "efficient": "vertex_1",
    "b_term_7": "helix_1",
    "author_b7": "M. Ivanova_1",
    "coauthor_b7": "K. Tanaka_1",
    "improving language understanding by generative pre-training": "LambdaUnit",
    "reasoning": "lattice_1",
    "b_term_8": "matrix_1",
    "author_b8": "S. Patel_1",
    "coauthor_b8": "L. Garcia_1",
    "bert: pre-training of deep bidirectional transformers for language understanding": "DeltaFrame",
    "tool-use": "tensor_1",
    "b_term_9": "vector_1",
    "author_b9": "A. Johansson_1",
    "coauthor_b9": "D. Kowalski_1",
    "language models are unsupervised multitask learners": "EpsilonNode",
    "b_term_10": "kernel_1",
    "author_b10": "W. Okafor_1",
    "coauthor_b10": "P. Dubois_1",
    "roberta: a robustly optimized bert pretraining approach": "RhoNet",
    "b_term_11": "quark_2",
    "author_b11": "J. Smith_2",
    "coauthor_b11": "R. Chen_2",
    "albert: a lite bert for self-supervised learning of language representations": "ZetaNet_1",
    "b_term_12": "prism_2",
    "author_b12": "M. Ivanova_2",
    "coauthor_b12": "K. Tanaka_2",
    "exploring the limits of transfer learning with a unified text-to-text transformer": "OmegaArch_1",
    "b_term_13": "nexus_2",
    "author_b13": "S. Patel_2",
    "coauthor_b13": "L. Garcia_2",
    "language models are few-shot learners": "KappaModel_1",
    "b_term_14": "vertex_2",
    "author_b14": "A. Johansson_2",
    "coauthor_b14": "D. Kowalski_2",
    "switch transformers: scaling to trillion parameter models": "SigmaBlock_1",
    "b_term_15": "helix_2",
    "author_b15": "W. Okafor_2",
    "coauthor_b15": "P. Dubois_2",
    "lora: low-rank adaptation of large language models": "PhiStack_1",
    "b_term_16": "lattice_2",
    "author_b16": "J. Smith_3",
    "coauthor_b16": "R. Chen_3",
    "training language models to follow instructions with human feedback": "ThetaCore_1",
    "b_term_17": "matrix_2",
    "author_b17": "M. Ivanova_3",
    "coauthor_b17": "K. Tanaka_3",
    "constitutional ai: harmlessness from ai feedback": "LambdaUnit_1",
    "b_term_18": "tensor_2",
    "author_b18": "S. Patel_3",
    "coauthor_b18": "L. Garcia_3",
    "llama: open and efficient foundation language models": "DeltaFrame_1",
    "b_term_19": "vector_2",
    "author_b19": "A. Johansson_3",
    "coauthor_b19": "D. Kowalski_3",
    "retrieval-augmented generation for knowledge-intensive nlp tasks": "EpsilonNode_1",
    "b_term_20": "kernel_2",
    "author_b20": "W. Okafor_3",
    "coauthor_b20": "P. Dubois_3",
    "toolformer: language models can teach themselves to use tools": "RhoNet_1"
  },
  "category_counters": {
    "keyword": 30,
    "author": 40,
    "venue": 1,
    "model": 20
  }
}