{
  "seed": 42,
  "forward": {
    "long short-term memory": "ZetaNet",
    "kyunghyun cho": "J. Smith",
    "b. v. merrienboer": "R. Chen",
    "çaglar gülçehre": "M. Ivanova",
    "conference on empirical methods in natural language processing": "AICONF",
    "learning phrase representations using rnn encoder–decoder for statistical machine translation": "OmegaArch",
    "i. sutskever": "K. Tanaka",
    "o. vinyals": "S. Patel",
    "quoc v. le": "L. Garcia",
    "neural information processing systems": "VISIONEX",
    "sequence to sequence learning with neural networks": "KappaModel",
    "dzmitry bahdanau": "A. Johansson",
    "yoshua bengio": "D. Kowalski",
    "international conference on learning representations": "LEARNCON",
    "neural machine translation by jointly learning to align and translate": "SigmaBlock",
    "thang luong": "W. Okafor",
    "hieu pham": "P. Dubois",
    "christopher d. manning": "J. Smith_1",
    "effective approaches to attention-based neural machine translation": "PhiStack",
    "ashish vaswani": "R. Chen_1",
    "noam shazeer": "M. Ivanova_1",
    "niki parmar": "K. Tanaka_1",
    "attention is all you need": "ThetaCore",
    "improving language understanding by generative pre-training": "LambdaUnit",
    "bert: pre-training of deep bidirectional transformers for language understanding": "DeltaFrame",
    "language models are unsupervised multitask learners": "EpsilonNode",
    "yinhan liu": "S. Patel_1",
    "myle ott": "L. Garcia_1",
    "naman goyal": "A. Johansson_1",
    "arxiv.org": "NEUROSYM",
    "roberta: a robustly optimized bert pretraining approach": "RhoNet",
    "zhenzhong lan": "D. Kowalski_1",
    "mingda chen": "W. Okafor_1",
    "sebastian goodman": "P. Dubois_1",
    "albert: a lite bert for self-supervised learning of language representations": "ZetaNet_1",
    "colin raffel": "J. Smith_2",
    "adam roberts": "R. Chen_2",
    "journal of machine learning research": "DATAFORGE",
    "exploring the limits of transfer learning with a unified text-to-text transformer": "OmegaArch_1",
    "tom b. brown": "M. Ivanova_2",
    "benjamin mann": "K. Tanaka_2",
    "nick ryder": "S. Patel_2",
    "language models are few-shot learners": "KappaModel_1",
    "w. fedus": "L. Garcia_2",
    "barret zoph": "A. Johansson_2",
    "switch transformers: scaling to trillion parameter models with simple and efficient sparsity": "SigmaBlock_1",
    "edward j. hu": "D. Kowalski_2",
    "yelong shen": "W. Okafor_2",
    "phillip wallis": "P. Dubois_2",
    "lora: low-rank adaptation of large language models": "PhiStack_1",
    "long ouyang": "J. Smith_3",
    "jeff wu": "R. Chen_3",
    "xu jiang": "M. Ivanova_3",
    "training language models to follow instructions with human feedback": "ThetaCore_1",
    "yuntao bai": "K. Tanaka_3",
    "saurav kadavath": "S. Patel_3",
    "sandipan kundu": "L. Garcia_3",
    "constitutional ai: harmlessness from ai feedback": "LambdaUnit_1",
    "hugo touvron": "A. Johansson_3",
    "thibaut lavril": "D. Kowalski_3",
    "gautier izacard": "W. Okafor_3",
    "llama: open and efficient foundation language models": "DeltaFrame_1",
    "niklas muennighoff": "P. Dubois_3",
    "alexander m. rush": "J. Smith_4",
    "b. barak": "R. Chen_4",
    "scaling data-constrained language models": "EpsilonNode_1",
    "patrick lewis": "M. Ivanova_4",
    "ethan perez": "K. Tanaka_4",
    "aleksandara piktus": "S. Patel_4",
    "retrieval-augmented generation for knowledge-intensive nlp tasks": "RhoNet_1",
    "toolformer: language models can teach themselves to use tools": "ZetaNet_2",
    "albert gu": "L. Garcia_4",
    "tri dao": "A. Johansson_4",
    "mamba: linear-time sequence modeling with selective state spaces": "OmegaArch_2",
    "karan goel": "D. Kowalski_4",
    "christopher r'e": "W. Okafor_4",
    "efficiently modeling long sequences with structured state spaces": "KappaModel_2",
    "daniel y. fu": "P. Dubois_4",
    "stefano ermon": "J. Smith_5",
    "flashattention: fast and memory-efficient exact attention with io-awareness": "SigmaBlock_2",
    "sheng shen": "R. Chen_5",
    "le hou": "M. Ivanova_5",
    "yan-quan zhou": "K. Tanaka_5",
    "mixture-of-experts meets instruction tuning: a winning combination for large language models": "PhiStack_2",
    "rafael rafailov": "S. Patel_5",
    "archit sharma": "L. Garcia_5",
    "e. mitchell": "A. Johansson_5",
    "direct preference optimization: your language model is secretly a reward model": "ThetaCore_2",
    "jason wei": "D. Kowalski_5",
    "xuezhi wang": "W. Okafor_5",
    "dale schuurmans": "P. Dubois_5",
    "chain of thought prompting elicits reasoning in large language models": "LambdaUnit_2",
    "shunyu yao": "J. Smith_6",
    "dian yu": "R. Chen_6",
    "jeffrey zhao": "M. Ivanova_6",
    "tree of thoughts: deliberate problem solving with large language models": "DeltaFrame_2",
    "yizhong wang": "K. Tanaka_6",
    "yeganeh kordi": "S. Patel_6",
    "swaroop mishra": "L. Garcia_6",
    "annual meeting of the association for computational linguistics": "SIGNALX",
    "self-instruct: aligning language models with self-generated instructions": "EpsilonNode_2",
    "suriya gunasekar": "A. Johansson_6",
    "yi zhang": "D. Kowalski_6",
    "j. aneja": "W. Okafor_6",
    "textbooks are all you need": "RhoNet_2"
  },
  "category_counters": {
    "model": 30,
    "author": 69,
    "venue": 6
  }
}