{"paper_id": "B_001", "title": "ZetaNet", "phase": "early", "background": "[TODO: fill manually]", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": [], "keywords": [], "year": null, "venue": null, "authors": null}
{"paper_id": "B_002", "title": "OmegaArch", "phase": "early", "background": "In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The enco", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_001"], "keywords": [], "year": null, "venue": "AICONF", "authors": ["J. Smith", "R. Chen", "M. Ivanova"]}
{"paper_id": "B_003", "title": "KappaModel", "phase": "early", "background": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_001"], "keywords": [], "year": null, "venue": "VISIONEX", "authors": ["K. Tanaka", "S. Patel", "L. Garcia"]}
{"paper_id": "B_004", "title": "SigmaBlock", "phase": "early", "background": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed re", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_003"], "keywords": [], "year": null, "venue": "LEARNCON", "authors": ["A. Johansson", "J. Smith", "D. Kowalski"]}
{"paper_id": "B_005", "title": "PhiStack", "phase": "early", "background": "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effecti", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_004"], "keywords": [], "year": null, "venue": "AICONF", "authors": ["W. Okafor", "P. Dubois", "J. Smith_1"]}
{"paper_id": "B_006", "title": "ThetaCore", "phase": "mid", "background": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_004", "B_005"], "keywords": [], "year": null, "venue": "VISIONEX", "authors": ["R. Chen_1", "M. Ivanova_1", "K. Tanaka_1"]}
{"paper_id": "B_007", "title": "LambdaUnit", "phase": "mid", "background": "[TODO: fill manually]", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_006"], "keywords": [], "year": null, "venue": null, "authors": null}
{"paper_id": "B_008", "title": "DeltaFrame", "phase": "mid", "background": "[TODO: fill manually]", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_006"], "keywords": [], "year": null, "venue": null, "authors": null}
{"paper_id": "B_009", "title": "EpsilonNode", "phase": "mid", "background": "[TODO: fill manually]", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_007"], "keywords": [], "year": null, "venue": null, "authors": null}
{"paper_id": "B_010", "title": "RhoNet", "phase": "mid", "background": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on th", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_008"], "keywords": [], "year": null, "venue": "NEUROSYM", "authors": ["S. Patel_1", "L. Garcia_1", "A. Johansson_1"]}
{"paper_id": "B_011", "title": "ZetaNet_1", "phase": "mid", "background": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two param", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_008"], "keywords": [], "year": null, "venue": "LEARNCON", "authors": ["D. Kowalski_1", "W. Okafor_1", "P. Dubois_1"]}
{"paper_id": "B_012", "title": "OmegaArch_1", "phase": "mid", "background": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and pract", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_008"], "keywords": [], "year": null, "venue": "DATAFORGE", "authors": ["J. Smith_2", "M. Ivanova_1", "R. Chen_2"]}
{"paper_id": "B_013", "title": "KappaModel_1", "phase": "late", "background": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of t", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_009"], "keywords": [], "year": null, "venue": "VISIONEX", "authors": ["M. Ivanova_2", "K. Tanaka_2", "S. Patel_2"]}
{"paper_id": "B_014", "title": "SigmaBlock_1", "phase": "late", "background": "In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_012", "B_013"], "keywords": [], "year": null, "venue": "DATAFORGE", "authors": ["L. Garcia_2", "A. Johansson_2", "M. Ivanova_1"]}
{"paper_id": "B_015", "title": "PhiStack_1", "phase": "late", "background": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example ", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_013"], "keywords": [], "year": null, "venue": "LEARNCON", "authors": ["D. Kowalski_2", "W. Okafor_2", "P. Dubois_2"]}
{"paper_id": "B_016", "title": "ThetaCore_1", "phase": "late", "background": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we sho", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_013"], "keywords": [], "year": null, "venue": "VISIONEX", "authors": ["J. Smith_3", "R. Chen_3", "M. Ivanova_3"]}
{"paper_id": "B_017", "title": "LambdaUnit_1", "phase": "late", "background": "As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules o", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_016"], "keywords": [], "year": null, "venue": "NEUROSYM", "authors": ["K. Tanaka_3", "S. Patel_3", "L. Garcia_3"]}
{"paper_id": "B_018", "title": "DeltaFrame_1", "phase": "late", "background": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_013"], "keywords": [], "year": null, "venue": "NEUROSYM", "authors": ["A. Johansson_3", "D. Kowalski_3", "W. Okafor_3"]}
{"paper_id": "B_019", "title": "EpsilonNode_1", "phase": "late", "background": "The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling lang", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_013", "B_018"], "keywords": [], "year": null, "venue": "VISIONEX", "authors": ["P. Dubois_3", "J. Smith_4", "R. Chen_4"]}
{"paper_id": "B_020", "title": "RhoNet_1", "phase": "late", "background": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_008", "B_013"], "keywords": [], "year": null, "venue": "VISIONEX", "authors": ["M. Ivanova_4", "K. Tanaka_4", "S. Patel_4"]}
{"paper_id": "B_021", "title": "ZetaNet_2", "phase": "late", "background": "[TODO: fill manually]", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_013"], "keywords": [], "year": null, "venue": null, "authors": null}
{"paper_id": "B_022", "title": "OmegaArch_2", "phase": "late", "background": "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state ", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_006"], "keywords": [], "year": null, "venue": "NEUROSYM", "authors": ["L. Garcia_4", "A. Johansson_4"]}
{"paper_id": "B_023", "title": "KappaModel_2", "phase": "late", "background": "A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long ", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_006"], "keywords": [], "year": null, "venue": "LEARNCON", "authors": ["L. Garcia_4", "D. Kowalski_4", "W. Okafor_4"]}
{"paper_id": "B_024", "title": "SigmaBlock_2", "phase": "late", "background": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not ach", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_006"], "keywords": [], "year": null, "venue": "VISIONEX", "authors": ["A. Johansson_4", "P. Dubois_4", "J. Smith_5"]}
{"paper_id": "B_025", "title": "PhiStack_2", "phase": "late", "background": "Sparse Mixture-of-Experts (MoE) is a neural architecture design that can be utilized to add learnable parameters to Large Language Models (LLMs) without increasing inference cost. Instruction tuning is a technique for training LLMs to follow instructions. We advocate combining these two approaches, ", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_014", "B_016"], "keywords": [], "year": null, "venue": "LEARNCON", "authors": ["R. Chen_5", "M. Ivanova_5", "K. Tanaka_5"]}
{"paper_id": "B_026", "title": "ThetaCore_2", "phase": "late", "background": "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_016"], "keywords": [], "year": null, "venue": "VISIONEX", "authors": ["S. Patel_5", "L. Garcia_5", "A. Johansson_5"]}
{"paper_id": "B_027", "title": "LambdaUnit_2", "phase": "late", "background": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simpl", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_013"], "keywords": [], "year": null, "venue": "VISIONEX", "authors": ["D. Kowalski_5", "W. Okafor_5", "P. Dubois_5"]}
{"paper_id": "B_028", "title": "DeltaFrame_2", "phase": "late", "background": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where i", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_027"], "keywords": [], "year": null, "venue": "VISIONEX", "authors": ["J. Smith_6", "R. Chen_6", "M. Ivanova_6"]}
{"paper_id": "B_029", "title": "EpsilonNode_2", "phase": "late", "background": "Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, theref", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_016"], "keywords": [], "year": null, "venue": "SIGNALX", "authors": ["K. Tanaka_6", "S. Patel_6", "L. Garcia_6"]}
{"paper_id": "B_030", "title": "RhoNet_2", "phase": "late", "background": "We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality\"data from the web (6B tokens) and synthetically generated te", "mechanism": "[TODO: fill manually]", "experiment": "[TODO: fill manually]", "results": {"metric_main": 0.0, "delta_vs_prev": 0.0, "extra": {}}, "dependencies": ["B_018", "B_019"], "keywords": [], "year": null, "venue": "NEUROSYM", "authors": ["A. Johansson_6", "D. Kowalski_6", "W. Okafor_6"]}
